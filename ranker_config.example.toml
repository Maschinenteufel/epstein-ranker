# Sample configuration for gpt_ranker.py

# Endpoint / model overrides
endpoint = "http://localhost:5555/v1"
api_format = "openai" # "auto", "openai", or "chat"
model = "qwen/qwen3-vl-30b"
api_key = "" # Optional API key (required for hosted providers like OpenRouter)
http_referer = "" # Optional HTTP-Referer header (recommended by OpenRouter)
x_title = "" # Optional X-Title header (recommended by OpenRouter)
processing_mode = "image" # "auto", "text", or "image"
justice_files_base_url = "https://www.justice.gov/epstein/files"
source_files_base_url = "" # Optional hosted base URL for local files (leave blank for auto /data/... derivation)
max_parallel_requests = 4
parallel_scheduling = "batch"  # auto | window | batch. batch = submit N, wait N, submit next N.
image_prefetch = 0  # Extra queued image tasks (used in window mode).
temperature = 0.0  # Temperature for model responses (0.0 = deterministic, higher = more random)
max_output_tokens = 900  # Cap completion length to prevent runaway generations
reasoning_effort = "low"  # Optional: "low", "medium", or "high" (omit if model doesn't support it)
sleep = 0.0
timeout = 1200.0  # HTTP request timeout in seconds (10 minutes)
                 # For very large documents (100K+ tokens), increase to 1200 (20 min) or higher
max_retries = 3
retry_backoff = 1.5
image_max_pages = 128
pdf_pages_per_image = 4  # 4 = 2x2 page collages per image block
pdf_part_pages = 128  # Split large PDFs into part records of this many pages
image_render_dpi = 120
image_detail = "low"  # auto | low | high. Lower detail reduces visual token usage.
image_output_format = "jpeg"  # png | jpeg
image_jpeg_quality = 75  # 1-95, only used for jpeg output
image_max_side = 1024  # Long-side resize cap in pixels (0 disables)
# debug_image_dir = "data/debug_images"  # Optional: save intermediate rendered/packed images + timing JSON

# Low-signal skipping safeguards
skip_low_quality = true
min_text_chars = 60
min_text_words = 12
min_alpha_ratio = 0.25
min_unique_word_ratio = 0.15
max_short_token_ratio = 0.6
min_avg_word_length = 3.0
min_long_word_count = 4
max_repeated_char_run = 40

# System prompt configuration
# Use a custom prompt file (optional - defaults to prompts/default_system_prompt.txt)
# prompt_file = "prompts/my_custom_prompt.txt"
# Or provide an inline system prompt (overrides prompt_file)
# system_prompt = "Your custom prompt here..."

# Chunking defaults
chunk_size = 1000
chunk_dir = "contrib"
chunk_manifest = "data/chunks.json"

# Energy / cost defaults (watts, USD/kWh)
power_watts = 100.0
electric_rate = 0.25
# Optionally provide expected hours if you want cost estimates independent of wall time.
# run_hours = 60.0

# Paths
input = "data/new_data/VOL00001"
input_glob = "*.pdf"  # Used when input is a directory
output = "data/epstein_ranked.csv"
json_output = "data/epstein_ranked.jsonl"
checkpoint = "data/.epstein_checkpoint"

# Optional isolated workspace mode for independent corpora.
# When set, outputs/checkpoint/chunks are routed under:
# <dataset_workspace_root>/<dataset_tag>/
# dataset_workspace_root = "data/workspaces"
# dataset_tag = "standardworks_epstein_files"
# dataset_source_label = "Epstein-Files GitHub (raw PDFs)"
# dataset_source_url = "https://github.com/yung-megafone/Epstein-Files"
# dataset_metadata_file = "data/dataset_profiles/standardworks_epstein_files.json"
# run_metadata_file can be omitted in workspace mode (auto-generated in metadata/run_metadata.json)
# Example: reference other JSONL files to skip duplicates
# known_json = ["data/master_results.jsonl", "contrib/epstein_ranked_0_1000.jsonl"]

# OpenRouter quick-start example:
# endpoint = "https://openrouter.ai/api/v1"
# api_format = "openai"
# model = "qwen/qwen3-vl-30b-a3b-instruct"
# api_key = "sk-or-..."
# http_referer = "https://your-app.example"
# x_title = "Epstein File Ranker"
