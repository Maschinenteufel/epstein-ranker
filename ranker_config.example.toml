# Sample configuration for gpt_ranker.py

# Endpoint / model overrides
endpoint = "http://localhost:5002/v1"
model = "openai/gpt-oss-120b"
temperature = 0.4  # Temperature for model responses (0.0 = deterministic, higher = more random)
reasoning_effort = "low"  # Optional: "low", "medium", or "high" (omit if model doesn't support it)
sleep = 0.5
timeout = 1200.0  # HTTP request timeout in seconds (10 minutes)
                 # For very large documents (100K+ tokens), increase to 1200 (20 min) or higher

# System prompt configuration
# Use a custom prompt file (optional - defaults to prompts/default_system_prompt.txt)
# prompt_file = "prompts/my_custom_prompt.txt"
# Or provide an inline system prompt (overrides prompt_file)
# system_prompt = "Your custom prompt here..."

# Chunking defaults
chunk_size = 1000
chunk_dir = "contrib"
chunk_manifest = "data/chunks.json"

# Energy / cost defaults (watts, USD/kWh)
power_watts = 100.0
electric_rate = 0.25
# Optionally provide expected hours if you want cost estimates independent of wall time.
# run_hours = 60.0

# Paths
input = "data/EPS_FILES_20K_NOV2026.csv"
output = "data/epstein_ranked.csv"
json_output = "data/epstein_ranked.jsonl"
checkpoint = "data/.epstein_checkpoint"
# Example: reference other JSONL files to skip duplicates
# known_json = ["data/master_results.jsonl", "contrib/epstein_ranked_0_1000.jsonl"]
